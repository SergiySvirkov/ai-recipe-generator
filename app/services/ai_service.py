# app/services/ai_service.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import re

class RecipeGenerator:
    """
    A service class to handle recipe generation using a fine-tuned transformer model.
    """
    def __init__(self, model_path="models/recipe-generator-distilgpt2"):
        """
        Initializes the generator by loading the fine-tuned model and tokenizer.

        Args:
            model_path (str): The path to the directory containing the saved model and tokenizer.
        """
        print(f"Loading model from: {model_path}")
        # Check if a GPU is available and set the device accordingly.
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")

        # Load the tokenizer and model from the specified path.
        # The .to(self.device) call moves the model to the GPU if available.
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device)
        print("Model and tokenizer loaded successfully.")

    def _build_prompt(self, ingredients, diet="any"):
        """
        Builds the input prompt string for the model based on user inputs.

        Args:
            ingredients (list): A list of ingredient strings.
            diet (str): The dietary restriction (e.g., 'vegan', 'vegetarian').

        Returns:
            str: A formatted prompt string.
        """
        # Join the list of ingredients into a single comma-separated string.
        ingredient_str = ", ".join(ingredients).lower()
        
        # Start the prompt with a clear instruction.
        # The prompt structure should mimic the data format used during fine-tuning.
        prompt = f"TITLE: A {diet} recipe with {ingredient_str} INGREDIENTS:"
        if diet == "any":
            prompt = f"TITLE: A recipe with {ingredient_str} INGREDIENTS:"
            
        return prompt

    def _parse_output(self, generated_text):
        """
        Parses the raw text output from the model into a structured dictionary.

        Args:
            generated_text (str): The raw string generated by the model.

        Returns:
            dict: A dictionary with 'title', 'ingredients', and 'instructions'.
        """
        # Use regular expressions to find the sections.
        # The re.DOTALL flag allows '.' to match newlines.
        title_match = re.search(r"TITLE:\s*(.*?)\s*INGREDIENTS:", generated_text, re.DOTALL)
        ingredients_match = re.search(r"INGREDIENTS:\s*(.*?)\s*INSTRUCTIONS:", generated_text, re.DOTALL)
        instructions_match = re.search(r"INSTRUCTIONS:\s*(.*)", generated_text, re.DOTALL)

        title = title_match.group(1).strip() if title_match else "Untitled Recipe"
        
        # Split ingredients by comma or newline for flexibility
        ingredients_text = ingredients_match.group(1).strip() if ingredients_match else ""
        ingredients = [ing.strip() for ing in re.split(r',|\n', ingredients_text) if ing.strip()]

        # Split instructions by periods or newlines
        instructions_text = instructions_match.group(1).strip() if instructions_match else ""
        instructions = [inst.strip() for inst in re.split(r'\.|\n', instructions_text) if inst.strip()]

        return {
            "title": title,
            "ingredients": ingredients,
            "instructions": instructions
        }

    def generate(self, ingredients, diet="any"):
        """
        Generates a recipe based on a list of ingredients and a dietary preference.

        Args:
            ingredients (list): A list of ingredient strings.
            diet (str): The dietary preference.

        Returns:
            dict: A structured recipe dictionary.
        """
        prompt = self._build_prompt(ingredients, diet)
        
        # Encode the prompt text into tokens and move to the selected device.
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # --- Generate Text ---
        # The model.generate() method is highly configurable.
        # These parameters are a good starting point for creative text generation.
        output_sequences = self.model.generate(
            input_ids=inputs['input_ids'],
            max_length=300,  # Maximum length of the generated text in tokens.
            temperature=0.9, # Higher temperature -> more randomness/creativity. Lower -> more deterministic.
            top_k=50,        # Considers only the top 50 most likely tokens at each step.
            top_p=0.95,      # Nucleus sampling: considers a cumulative probability mass of 95%.
            num_return_sequences=1, # We only want one recipe.
            pad_token_id=self.tokenizer.eos_token_id # Set padding token to avoid warnings.
        )
        
        # Decode the generated tokens back into a string.
        generated_text = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)
        
        # Parse the raw text into a clean, structured format.
        return self._parse_output(generated_text)

